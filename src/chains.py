import os
import torch
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel

# --- 1. CONFIGURA√á√ÉO DE CAMINHOS ---
base_dir = os.path.dirname(os.path.abspath(__file__)) 
adapter_path = os.path.join(base_dir, "..", "models")
csv_path = os.path.join(base_dir, "..", "data", "base_medica.csv")

print("‚öôÔ∏è Carregando Sistema (Modo Manual RAG)...")

# --- 2. CARREGAR MODELO ---
try:
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float32,
        device_map="cpu"
    )
    
    # Carrega Fine-Tuning
    model = PeftModel.from_pretrained(base_model, adapter_path)
    print("‚úÖ Modelo carregado.")
except Exception as e:
    print(f"‚ùå Erro no modelo: {e}")
    exit()

# --- 3. BANCO DE DADOS (MEM√ìRIA OU CSV) ---
# Se o CSV existir, usa ele. Se n√£o, usa mem√≥ria.
protocolos_memoria = [
    "PROTOCOLO DOR DE CABE√áA: Dipirona 1g se leve. Sumatriptano se enxaqueca.",
    "PROTOCOLO IAM (INFARTO): Monitoriza√ß√£o, Oxig√™nio, AAS 300mg e Clopidogrel 300mg.",
    "SEGURAN√áA: Nunca prescrever controlados sem valida√ß√£o humana."
]

print("üìö Indexando dados...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Tenta carregar do CSV se poss√≠vel (b√¥nus), sen√£o usa mem√≥ria
if os.path.exists(csv_path):
    from langchain_community.document_loaders import CSVLoader
    loader = CSVLoader(csv_path, encoding='utf-8')
    docs = loader.load()
    vector_db = FAISS.from_documents(docs, embeddings)
    print(f"‚úÖ Base CSV carregada ({len(docs)} itens).")
else:
    vector_db = FAISS.from_texts(protocolos_memoria, embeddings)
    print("‚úÖ Base de mem√≥ria carregada.")

# --- 4. A L√ìGICA RAG (MANUAL) ---
# Aqui substitu√≠mos o 'RetrievalQA' por l√≥gica pura. Funciona sempre.
def consultar_assistente(pergunta):
    
    # PASSO A: Busca (Retrieval)
    # Busca os 2 documentos mais parecidos com a pergunta
    docs_encontrados = vector_db.similarity_search(pergunta, k=2)
    
    # Junta o texto dos documentos numa string s√≥
    contexto = "\n".join([doc.page_content for doc in docs_encontrados])
    
    # PASSO B: Constru√ß√£o do Prompt (Augmentation)
    prompt_final = f"""<|system|>
Voc√™ √© um assistente m√©dico. Responda √† d√∫vida usando APENAS o contexto abaixo.
CONTEXTO:
{contexto}
</s>
<|user|>
{pergunta}
</s>
<|assistant|>
"""
    
    # PASSO C: Gera√ß√£o (Generation)
    inputs = tokenizer(prompt_final, return_tensors="pt")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=False, # Determin√≠stico
        repetition_penalty=1.2
    )
    
    resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Limpeza para pegar s√≥ a resposta
    if "<|assistant|>" in resposta_completa:
        return resposta_completa.split("<|assistant|>\n")[-1]
    return resposta_completa

# --- 5. LOOP DE INTERA√á√ÉO ---
if __name__ == "__main__":
    print("\n" + "="*40)
    print("üè• CHAT M√âDICO ATIVO (CTRL+C para sair)")
    print("="*40)
    while True:
        try:
            p = input("\nüë®‚Äç‚öïÔ∏è Pergunta: ")
            if p.lower() in ['sair', 'exit']: break
            
            print("üîç Pesquisando nos protocolos...")
            res = consultar_assistente(p)
            print(f"ü§ñ Resposta: {res}")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"Erro: {e}")